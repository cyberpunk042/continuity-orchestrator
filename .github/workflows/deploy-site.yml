name: Deploy Site

# Build and deploy static site to GitHub Pages
# Triggered ONLY for manual dispatch or template/site code changes
# NOT triggered by state/audit/public changes (cron handles those)

on:
  workflow_dispatch:
    inputs:
      deploy:
        description: 'Deploy to GitHub Pages'
        required: false
        default: 'true'
        type: boolean
  workflow_call:
  push:
    branches: [main]
    paths:
      # Only trigger for code/template changes, NOT state changes
      # Cron workflow handles all automatic deployments
      - 'templates/**'
      - 'src/site/**'
      - 'content/**'
      # Explicitly exclude (cron handles these):
      # - 'state/**'
      # - 'audit/**' 
      # - 'public/**'

# Sets permissions for GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment â€” queue behind renew/cron (never cancel them)
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      # Use minimal Python setup - site generation only needs core deps
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      
      # Install with uv â€” fast enough to install the full package
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv pip install --system -e . --quiet
      
      - name: Build site
        env:
          RENEWAL_TRIGGER_TOKEN: ${{ secrets.RENEWAL_TRIGGER_TOKEN }}
          CONTENT_ENCRYPTION_KEY: ${{ secrets.CONTENT_ENCRYPTION_KEY }}
        run: |
          python -m src.main build-site --output public
      
      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./public

  deploy:
    if: github.event.inputs.deploy != 'false'
    runs-on: ubuntu-latest
    needs: build
    
    environment:
      name: github-pages
      url: ${{ steps.deploy3.outputs.page_url || steps.deploy2.outputs.page_url || steps.deployment.outputs.page_url }}
    
    steps:
      - name: Checkout (for cleanup script)
        uses: actions/checkout@v4
        with:
          sparse-checkout: scripts
          sparse-checkout-cone-mode: true

      - name: Clear stuck Pages deployments
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python3 scripts/clear_stuck_deployments.py && sleep 2

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
        continue-on-error: true

      - name: Backoff before retry
        if: steps.deployment.outcome == 'failure'
        run: echo "âš ï¸ Deploy attempt 1 failed â€” waiting 30s..." && sleep 30

      - name: Deploy to GitHub Pages (attempt 2)
        if: steps.deployment.outcome == 'failure'
        id: deploy2
        uses: actions/deploy-pages@v4
        continue-on-error: true

      - name: Backoff before final retry
        if: steps.deployment.outcome == 'failure' && steps.deploy2.outcome == 'failure'
        run: echo "âš ï¸ Deploy attempt 2 failed â€” waiting 90s..." && sleep 90

      - name: Deploy to GitHub Pages (attempt 3)
        if: steps.deployment.outcome == 'failure' && steps.deploy2.outcome == 'failure'
        id: deploy3
        uses: actions/deploy-pages@v4
      
      # Archive step - archive all key pages if enabled
      - name: Archive to Wayback Machine
        if: vars.ARCHIVE_ENABLED == 'true'
        env:
          ARCHIVE_URL: ${{ vars.ARCHIVE_URL }}
          GITHUB_REPOSITORY: ${{ github.repository }}
        run: |
          # Determine base URL
          if [ -n "$ARCHIVE_URL" ]; then
            BASE_URL="${ARCHIVE_URL%/}"
          else
            OWNER="${GITHUB_REPOSITORY%/*}"
            REPO="${GITHUB_REPOSITORY#*/}"
            BASE_URL="https://${OWNER}.github.io/${REPO}"
          fi
          
          echo "ðŸ“¦ Archiving site to archive.org..."
          
          # Wait for GitHub Pages propagation
          sleep 10
          
          # Collect URLs to archive from sitemap or fallback list
          URLS=()
          SITEMAP_URL="${BASE_URL}/sitemap.xml"
          SITEMAP=$(curl -s --max-time 10 "$SITEMAP_URL" 2>/dev/null || true)
          
          if echo "$SITEMAP" | grep -q '<loc>'; then
            # Extract URLs from sitemap
            while IFS= read -r loc; do
              URLS+=("$loc")
            done < <(echo "$SITEMAP" | grep -oP '(?<=<loc>)[^<]+')
            echo "  Found ${#URLS[@]} URL(s) from sitemap.xml"
          else
            # Fallback: just the index
            URLS=("${BASE_URL}/")
            echo "  No sitemap found, archiving index only"
          fi
          
          # Submit each URL (best effort, don't fail workflow)
          SUCCESS=0
          TOTAL=${#URLS[@]}
          for URL in "${URLS[@]}"; do
            LABEL="${URL#$BASE_URL/}"
            [ -z "$LABEL" ] && LABEL="index"
            printf "  [%d/%d] %s..." "$((SUCCESS + 1))" "$TOTAL" "$LABEL"
            
            RESULT=$(curl -s -o /dev/null -w "%{http_code}" --max-time 60 \
              -H "User-Agent: Mozilla/5.0" \
              "https://web.archive.org/save/$URL" 2>/dev/null || echo "timeout")
            
            if [ "$RESULT" = "200" ] || [ "$RESULT" = "302" ]; then
              echo " âœ“"
              SUCCESS=$((SUCCESS + 1))
            else
              echo " ($RESULT)"
            fi
            
            # Rate limit: ~3/min for anonymous
            sleep 5
          done
          
          echo "  ðŸ“¦ Archived $SUCCESS/$TOTAL pages"
